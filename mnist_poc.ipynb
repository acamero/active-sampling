{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7762d8ac-a4dc-4e60-88ae-d9866141d439",
   "metadata": {},
   "source": [
    "# MNIST Active Learning POC\n",
    "\n",
    "General idea: Achieve the highest possible accuracy, with the lowest amount of train data using (supervised learning).\n",
    "\n",
    "We start from a pool of unlabeled data and a test set. Then, we itiretaively query new samples to be annotated, and re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbea19d-537b-41d5-bd32-bcc59a336edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 11:50:43.826818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-22 11:50:43.826838: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c78bd-bbc4-486a-a7f9-b7da5ed04cb8",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2162f159-958b-4d27-a258-d49a1365283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original training examples: 60000\n",
      "Number of original test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f2f57-aff3-4e92-a7bf-905db352a431",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3630cd2f-c691-4566-a399-e4e4693177d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 11:50:51.155696: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-22 11:50:51.155754: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-22 11:50:51.155778: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (eoc-001261l): /proc/driver/nvidia/version does not exist\n",
      "2022-04-22 11:50:51.156092: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-22 11:50:51.325205: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3946 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.2042 - val_sparse_categorical_accuracy: 0.9438\n",
      "Epoch 2/6\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1836 - sparse_categorical_accuracy: 0.9479 - val_loss: 0.1527 - val_sparse_categorical_accuracy: 0.9581\n",
      "Epoch 3/6\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1321 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.1266 - val_sparse_categorical_accuracy: 0.9649\n",
      "Epoch 4/6\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1024 - sparse_categorical_accuracy: 0.9716 - val_loss: 0.1108 - val_sparse_categorical_accuracy: 0.9688\n",
      "Epoch 5/6\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0833 - sparse_categorical_accuracy: 0.9763 - val_loss: 0.1048 - val_sparse_categorical_accuracy: 0.9696\n",
      "Epoch 6/6\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0690 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.0972 - val_sparse_categorical_accuracy: 0.9721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa86659daf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the batch size will be used as the number of new images to annotate\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "baseline_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "baseline_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "baseline_model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=6,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1817bd-d8d3-4ea8-9c9a-f507d9fedf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 591us/step - loss: 0.0906 - sparse_categorical_accuracy: 0.9724\n",
      "SparseCategoricalAccuracy 0.9724000096321106\n"
     ]
    }
   ],
   "source": [
    "_, baseline_metrics = baseline_model.evaluate(\n",
    "    x=x_test,\n",
    "    y=y_test,\n",
    ")\n",
    "\n",
    "print(\"SparseCategoricalAccuracy\", baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec713d-1264-4e16-88b9-73eb02f1e145",
   "metadata": {},
   "source": [
    "## Randomly Added Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8edbe0e5-e28d-4e2e-853f-132f445591e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of annotated images 600 out of 60000\n"
     ]
    }
   ],
   "source": [
    "INIT_SIZE = int(len(x_train) * 0.01)\n",
    "print(\"Initial number of annotated images\", INIT_SIZE, \"out of\", len(x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "38d1d4ea-6230-439a-9e1d-dd4fbd561b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of initial set of annotated images 600\n",
      "Epoch 1/6\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 2.2005 - sparse_categorical_accuracy: 0.1979 - val_loss: 1.9783 - val_sparse_categorical_accuracy: 0.3833\n",
      "Epoch 2/6\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7686 - sparse_categorical_accuracy: 0.6062 - val_loss: 1.6194 - val_sparse_categorical_accuracy: 0.6500\n",
      "Epoch 3/6\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.4125 - sparse_categorical_accuracy: 0.7333 - val_loss: 1.3151 - val_sparse_categorical_accuracy: 0.7750\n",
      "Epoch 4/6\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.1149 - sparse_categorical_accuracy: 0.8083 - val_loss: 1.0789 - val_sparse_categorical_accuracy: 0.7667\n",
      "Epoch 5/6\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8911 - sparse_categorical_accuracy: 0.8458 - val_loss: 0.9079 - val_sparse_categorical_accuracy: 0.8083\n",
      "Epoch 6/6\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7268 - sparse_categorical_accuracy: 0.8646 - val_loss: 0.7867 - val_sparse_categorical_accuracy: 0.8250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7fcf6a490>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_ix = list()\n",
    "\n",
    "annotated_ix = annotated_ix + np.random.choice(range(len(x_train)), \n",
    "                                               size=INIT_SIZE, replace=False).tolist()\n",
    "\n",
    "x_train_annotated = x_train[annotated_ix]    \n",
    "y_train_annotated = y_train[annotated_ix]\n",
    "\n",
    "print(\"Size of initial set of annotated images\", len(x_train_annotated))\n",
    "\n",
    "random_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "random_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "random_model.fit(\n",
    "    x=x_train_annotated,\n",
    "    y=y_train_annotated,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=6,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee275cfe-4d3f-44d5-a42a-74566cdf682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 642us/step - loss: 0.7559 - sparse_categorical_accuracy: 0.8217\n",
      "SparseCategoricalAccuracy 0.8216999769210815\n"
     ]
    }
   ],
   "source": [
    "_, metrics = random_model.evaluate(\n",
    "    x=x_test,\n",
    "    y=y_test,\n",
    ")\n",
    "\n",
    "print(\"SparseCategoricalAccuracy\", metrics)\n",
    "\n",
    "random_metrics = [metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6df2f027-46d9-4702-aa58-2ac9e11aa850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of set of annotated images 856\n",
      "Epoch 1/6\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.1929 - sparse_categorical_accuracy: 0.2339 - val_loss: 1.8498 - val_sparse_categorical_accuracy: 0.4884\n",
      "Epoch 2/6\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6140 - sparse_categorical_accuracy: 0.6345 - val_loss: 1.3936 - val_sparse_categorical_accuracy: 0.7267\n",
      "Epoch 3/6\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1671 - sparse_categorical_accuracy: 0.7997 - val_loss: 1.0714 - val_sparse_categorical_accuracy: 0.7907\n",
      "Epoch 4/6\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8744 - sparse_categorical_accuracy: 0.8231 - val_loss: 0.8338 - val_sparse_categorical_accuracy: 0.8256\n",
      "Epoch 5/6\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6809 - sparse_categorical_accuracy: 0.8655 - val_loss: 0.7041 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 6/6\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5534 - sparse_categorical_accuracy: 0.8816 - val_loss: 0.6355 - val_sparse_categorical_accuracy: 0.8198\n",
      "313/313 [==============================] - 0s 612us/step - loss: 0.6121 - sparse_categorical_accuracy: 0.8374\n",
      "SparseCategoricalAccuracy 0.8374000191688538\n",
      "Current size of set of annotated images 984\n",
      "Epoch 1/6\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 2.0893 - sparse_categorical_accuracy: 0.3456 - val_loss: 1.7373 - val_sparse_categorical_accuracy: 0.6396\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.4902 - sparse_categorical_accuracy: 0.7243 - val_loss: 1.2831 - val_sparse_categorical_accuracy: 0.7665\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.0757 - sparse_categorical_accuracy: 0.8183 - val_loss: 0.9541 - val_sparse_categorical_accuracy: 0.7716\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7963 - sparse_categorical_accuracy: 0.8412 - val_loss: 0.7896 - val_sparse_categorical_accuracy: 0.7919\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6119 - sparse_categorical_accuracy: 0.8882 - val_loss: 0.6821 - val_sparse_categorical_accuracy: 0.8071\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4993 - sparse_categorical_accuracy: 0.8983 - val_loss: 0.6183 - val_sparse_categorical_accuracy: 0.8223\n",
      "313/313 [==============================] - 0s 668us/step - loss: 0.5547 - sparse_categorical_accuracy: 0.8549\n",
      "SparseCategoricalAccuracy 0.8549000024795532\n",
      "Current size of set of annotated images 1112\n",
      "Epoch 1/6\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2.0529 - sparse_categorical_accuracy: 0.3791 - val_loss: 1.6507 - val_sparse_categorical_accuracy: 0.6457\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4182 - sparse_categorical_accuracy: 0.6873 - val_loss: 1.1066 - val_sparse_categorical_accuracy: 0.7937\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9739 - sparse_categorical_accuracy: 0.7930 - val_loss: 0.8044 - val_sparse_categorical_accuracy: 0.8296\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7138 - sparse_categorical_accuracy: 0.8380 - val_loss: 0.6521 - val_sparse_categorical_accuracy: 0.8341\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5576 - sparse_categorical_accuracy: 0.8718 - val_loss: 0.5489 - val_sparse_categorical_accuracy: 0.8520\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4633 - sparse_categorical_accuracy: 0.9010 - val_loss: 0.5257 - val_sparse_categorical_accuracy: 0.8520\n",
      "313/313 [==============================] - 0s 560us/step - loss: 0.5170 - sparse_categorical_accuracy: 0.8599\n",
      "SparseCategoricalAccuracy 0.8598999977111816\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    ix_pool = np.delete(range(len(x_train)), annotated_ix)\n",
    "\n",
    "    annotated_ix = annotated_ix + np.random.choice(ix_pool, \n",
    "                                                   size=BATCH_SIZE, replace=False).tolist()\n",
    "\n",
    "    x_train_annotated = x_train[annotated_ix]    \n",
    "    y_train_annotated = y_train[annotated_ix]\n",
    "\n",
    "    print(\"Current size of set of annotated images\", len(x_train_annotated))\n",
    "\n",
    "    random_model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    random_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    random_model.fit(\n",
    "        x=x_train_annotated,\n",
    "        y=y_train_annotated,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=6,\n",
    "        validation_split=0.2,\n",
    "    )\n",
    "\n",
    "    _, metrics = random_model.evaluate(\n",
    "        x=x_test,\n",
    "        y=y_test,\n",
    "    )\n",
    "\n",
    "    print(\"SparseCategoricalAccuracy\", metrics)\n",
    "\n",
    "    random_metrics.append(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92503ddb-8e78-47e5-b3da-295969b92163",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6238aea2-f451-42d4-a0a2-f9a1f69ad4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of annotated images 600 out of 60000\n"
     ]
    }
   ],
   "source": [
    "x_train_flat = np.reshape(\n",
    "    x_train, \n",
    "    (len(x_train), x_train.shape[1] * x_train.shape[2] * x_train.shape[3])\n",
    ")\n",
    "\n",
    "# TODO: the number of cluster should be adjusted using internal/external validation metrics\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(x_train_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0803015f-f947-4cfc-93ea-99e61f4b5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_ratios(\n",
    "    labels,\n",
    "    sum_up_to):\n",
    "    \"\"\" Given an array of classes (labels), returns an array of\n",
    "    elements per class (ratio) that sum up to a given number.\n",
    "    In other words, the sum of all returned elements is equal to\n",
    "    'sum_up_to'.\n",
    "    \"\"\"\n",
    "    labels_counts = collections.Counter(labels)\n",
    "    # print(\"Number of elements per cluster:\", dict(labels_counts))\n",
    "\n",
    "    # select a repersentative initial set of images to be annotated\n",
    "    counts = list()\n",
    "    for i in range(len(labels_counts)):\n",
    "        counts.append(labels_counts[i])\n",
    "\n",
    "    counts = np.array(counts)\n",
    "    _min = counts.min() if counts.min() > 0 else 1\n",
    "    \n",
    "    counts = counts / _min\n",
    "    counts = counts / counts.sum()\n",
    "    counts = counts * sum_up_to\n",
    "    counts = counts.astype(int)\n",
    "\n",
    "    # to avoid smalls clusters to be left out, we add at least one element per cluster\n",
    "    counts[counts == 0] = 1\n",
    "\n",
    "    while counts.sum() != sum_up_to:\n",
    "        if counts.sum() > sum_up_to:\n",
    "            counts[counts.argmax()] = counts[counts.argmax()] - 1\n",
    "        elif counts.sum() < sum_up_to:\n",
    "            counts[counts.argmin()] = counts[counts.argmin()] + 1\n",
    "            \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5787f012-6d4e-4d6d-be5c-1ec7e8303c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images by class adjusted to sum up to the initial amount of annotated data [14 19 19 10 10 19 10  9  9  9]\n",
      "Size of initial set of annotated images 600\n"
     ]
    }
   ],
   "source": [
    "# we will use the number of elements per cluster to select the initial batch\n",
    "counts = norm_ratios(kmeans.labels_, INIT_SIZE)\n",
    "print(\"Images by class adjusted to sum up to the initial amount of annotated data\",\n",
    "      counts_active)\n",
    "\n",
    "set_ix = list()\n",
    "for i in range(len(counts)):\n",
    "    pos = np.where(kmeans.labels_ == i)[0]\n",
    "    set_ix = set_ix + np.random.choice(pos, size=counts[i], replace=False).tolist()\n",
    "    \n",
    "annotated_ix = set_ix.copy()\n",
    "\n",
    "x_train_annotated = x_train[annotated_ix]    \n",
    "y_train_annotated = y_train[annotated_ix]\n",
    "\n",
    "print(\"Size of initial set of annotated images\", len(x_train_annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aefa84cb-53d3-4473-b7e9-58c8641cb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 2.1758 - sparse_categorical_accuracy: 0.2208 - val_loss: 2.6889 - val_sparse_categorical_accuracy: 0.2167\n",
      "Epoch 2/6\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7168 - sparse_categorical_accuracy: 0.6229 - val_loss: 2.6445 - val_sparse_categorical_accuracy: 0.3667\n",
      "Epoch 3/6\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.3550 - sparse_categorical_accuracy: 0.7312 - val_loss: 2.7305 - val_sparse_categorical_accuracy: 0.3917\n",
      "Epoch 4/6\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0777 - sparse_categorical_accuracy: 0.7729 - val_loss: 2.8624 - val_sparse_categorical_accuracy: 0.4000\n",
      "Epoch 5/6\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8680 - sparse_categorical_accuracy: 0.8229 - val_loss: 2.8978 - val_sparse_categorical_accuracy: 0.4083\n",
      "Epoch 6/6\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7140 - sparse_categorical_accuracy: 0.8604 - val_loss: 2.9275 - val_sparse_categorical_accuracy: 0.4333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7fe1115e0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "active_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "active_model.fit(\n",
    "    x=x_train_annotated,\n",
    "    y=y_train_annotated,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=6,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d022236-cbb4-47ed-bab0-402474dfca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 627us/step - loss: 1.2635 - sparse_categorical_accuracy: 0.7161\n",
      "SparseCategoricalAccuracy 0.7160999774932861\n"
     ]
    }
   ],
   "source": [
    "_, metrics = active_model.evaluate(\n",
    "    x=x_test,\n",
    "    y=y_test,\n",
    ")\n",
    "\n",
    "print(\"SparseCategoricalAccuracy\", metrics)\n",
    "\n",
    "active_metrics = [metrics]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba113f-d880-4cd7-9392-5c811ea70ea9",
   "metadata": {},
   "source": [
    "**NOTE:** Should we use the test data to check instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fee5514c-6326-4c22-82e3-ffa52c4d00a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclasification by class adjusted to sum up to a batch [ 7 11  6 10  9  9  6  9  6 55]\n",
      "Adding 128 annotated images\n",
      "Current size of set of annotated images 728\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9010 - sparse_categorical_accuracy: 0.8247 - val_loss: 1.4032 - val_sparse_categorical_accuracy: 0.6096\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6810 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.9338 - val_sparse_categorical_accuracy: 0.6849\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5009 - sparse_categorical_accuracy: 0.8883 - val_loss: 0.7257 - val_sparse_categorical_accuracy: 0.8014\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4353 - sparse_categorical_accuracy: 0.9244 - val_loss: 0.6311 - val_sparse_categorical_accuracy: 0.8082\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3523 - sparse_categorical_accuracy: 0.9313 - val_loss: 0.5819 - val_sparse_categorical_accuracy: 0.8151\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.9364 - val_loss: 0.5666 - val_sparse_categorical_accuracy: 0.8082\n",
      "313/313 [==============================] - 0s 552us/step - loss: 0.5262 - sparse_categorical_accuracy: 0.8434\n",
      "SparseCategoricalAccuracy 0.8434000015258789\n",
      "Misclasification by class adjusted to sum up to a batch [15 17  7 19 17 11  8  8  7 19]\n",
      "Adding 128 annotated images\n",
      "Current size of set of annotated images 856\n",
      "Epoch 1/6\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3029 - sparse_categorical_accuracy: 0.9284 - val_loss: 0.4552 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 2/6\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.9503 - val_loss: 0.4279 - val_sparse_categorical_accuracy: 0.8488\n",
      "Epoch 3/6\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2259 - sparse_categorical_accuracy: 0.9620 - val_loss: 0.4075 - val_sparse_categorical_accuracy: 0.8488\n",
      "Epoch 4/6\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2005 - sparse_categorical_accuracy: 0.9678 - val_loss: 0.3897 - val_sparse_categorical_accuracy: 0.8663\n",
      "Epoch 5/6\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.1781 - sparse_categorical_accuracy: 0.9795 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.8605\n",
      "Epoch 6/6\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9795 - val_loss: 0.3602 - val_sparse_categorical_accuracy: 0.8605\n",
      "313/313 [==============================] - 0s 552us/step - loss: 0.4231 - sparse_categorical_accuracy: 0.8732\n",
      "SparseCategoricalAccuracy 0.873199999332428\n",
      "Misclasification by class adjusted to sum up to a batch [ 8 33  3 24 20  8 16 12  4]\n",
      "Adding 128 annotated images\n",
      "Current size of set of annotated images 984\n",
      "Epoch 1/6\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1744 - sparse_categorical_accuracy: 0.9682 - val_loss: 0.3589 - val_sparse_categorical_accuracy: 0.8782\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9733 - val_loss: 0.3385 - val_sparse_categorical_accuracy: 0.8883\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1368 - sparse_categorical_accuracy: 0.9822 - val_loss: 0.3233 - val_sparse_categorical_accuracy: 0.9036\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1233 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.3211 - val_sparse_categorical_accuracy: 0.9036\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1111 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.3219 - val_sparse_categorical_accuracy: 0.8782\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1018 - sparse_categorical_accuracy: 0.9936 - val_loss: 0.3162 - val_sparse_categorical_accuracy: 0.8832\n",
      "313/313 [==============================] - 0s 631us/step - loss: 0.3907 - sparse_categorical_accuracy: 0.8842\n",
      "SparseCategoricalAccuracy 0.8841999769210815\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    preds = active_model.predict(x_train_annotated)\n",
    "    preds_class = np.argmax(preds, axis=1)\n",
    "    misclassified_relative = np.where((preds_class - y_train_annotated)!=0)\n",
    "    misclassified_pos = np.take(annotated_ix, misclassified_relative)\n",
    "    misclassified_cluster_no = np.take(kmeans.labels_, misclassified_pos)\n",
    "\n",
    "    # compute ratio of misclassified classes\n",
    "    counts_active = norm_ratios(misclassified_cluster_no.ravel(), BATCH_SIZE)\n",
    "    print(\"Misclasification by class adjusted to sum up to a batch\", counts_active)\n",
    "\n",
    "    # select new images to annotate with the given ratios\n",
    "    ix_pool = np.delete(kmeans.labels_, annotated_ix)\n",
    "\n",
    "    set_ix = list()\n",
    "    for i in range(len(counts_active)):\n",
    "        pos = np.where(ix_pool == i)[0]\n",
    "        set_ix = set_ix + np.random.choice(pos, size=counts_active[i], replace=False).tolist()\n",
    "\n",
    "    print(\"Adding\", len(set_ix), \"annotated images\")\n",
    "\n",
    "    annotated_ix = annotated_ix + set_ix\n",
    "\n",
    "    x_train_annotated = x_train[annotated_ix]    \n",
    "    y_train_annotated = y_train[annotated_ix]\n",
    "\n",
    "    print(\"Current size of set of annotated images\", len(x_train_annotated))\n",
    "    \n",
    "    active_model.fit(\n",
    "        x=x_train_annotated,\n",
    "        y=y_train_annotated,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=6,\n",
    "        validation_split=0.2,\n",
    "    )\n",
    "\n",
    "    _, metrics = active_model.evaluate(\n",
    "        x=x_test,\n",
    "        y=y_test,\n",
    "    )\n",
    "\n",
    "    print(\"SparseCategoricalAccuracy\", metrics)\n",
    "\n",
    "    active_metrics.append(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed9602-04fc-4c2a-b0a4-1128e8d6a2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "356c1857-b636-4242-95b1-6da9ae294606",
   "metadata": {},
   "source": [
    "## Results Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8852fc-b0f9-4159-935d-25398502ccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
